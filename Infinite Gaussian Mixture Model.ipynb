{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinite Gaussian Mixture Model\n",
    "\n",
    "The paper presents a Markov Chain Monte Carlo method for inference based on an Infinite Gaussian Mixture Model. The data is assumed to have been generated from a countably infinite Mixture of Gaussians. The MCMC method gives us the parameters of the Gaussian components that generated the data. One might wonder how the method can possibly tell us infinitely many parameters. This is not a concern because at any time, we'll see only finite amount of data. The method will give us only the relevant parameters.\n",
    "\n",
    "This method is applicable when one wants to cluster data without knowing the number of clusters in advance. In this implementation, the observations will be one-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "from sampler import *\n",
    "import pprint\n",
    "\n",
    "# Generate test data.\n",
    "y = np.concatenate([np.random.normal(-20, 1, 500),\n",
    "                    np.random.normal(0, 1, 3000),\n",
    "                    np.random.normal(20, 1, 1000)]).reshape(-1, 1)\n",
    "mu_y = np.mean(y)\n",
    "var_y = np.var(y)\n",
    "\n",
    "c = np.array([0]*500 + [1]*3000 + [2]*1000)\n",
    "n = len(c)\n",
    "plt.hist(y, bins=50)\n",
    "plt.show()\n",
    "\n",
    "k = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that $k$, the number of mixture components was known. The model parameters would be $k$ means and $k$ precisions for the Gaussians, and the mixture weights. \n",
    "\n",
    "Means are denoted by $\\mu_1, \\cdots, \\mu_k$. Precisions by $s_1, \\cdots, s_k$. Finally, the mixture weights are denoted by $\\pi_1, \\cdots, \\pi_k$.\n",
    "\n",
    "\\begin{equation}\n",
    "p(y | \\mu_1, \\cdots, \\mu_k, s_1, \\cdots, s_k, \\pi_1, \\cdots, \\pi_k) = \\sum_{i = 1}^k \\pi_i \\mathcal{N}(\\mu_i, \\frac{1}{s_i})\n",
    "\\end{equation} \n",
    "\n",
    "The model parameters have hyperparameters associated with them. Parameters $\\mu_i$ are controlled by two hyperparameters $\\lambda$ and $r$. Parameters $s_i$ are controlled by $\\beta$ and $\\omega$. The mixture weights are controlled by a single hyperparameter $\\alpha$. \n",
    "\n",
    "The Bayes Net, with the priors on the hyperparameters is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with pm.Model() as finiteGMMmodel:\\n    # TODO: Distribution parameters may be wrong here.\\n    lambda_ = pm.Normal('lambda', mu=mu_y, tau=1/var_y)\\n    r_ = pm.Gamma('r', mu=1, sigma=1/var_y)\\n    mu_ = pm.Normal('mu', mu=lambda_, tau=r_, shape=k)\\n    beta_ = pm.InverseGamma('beta', mu=1, sigma=1)\\n    omega_ = pm.Gamma('omega', alpha=1, beta=var_y)\\n    s_ = pm.Gamma('s', mu=beta_, sigma=1/omega_, shape=k)\\n    alpha_ = pm.Gamma('alpha', alpha=1, beta=1)\\n    pi_ = pm.Dirichlet('pi', a=np.ones(k) * (alpha_ / k))\\n    c_ = pm.Categorical('c', p=pi_)\\n    y_ = pm.Normal('y', mu=mu_[c_], tau=s_[c_], observed=y)\\n\\npm.model_to_graphviz(finiteGMMmodel)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "'''with pm.Model() as finiteGMMmodel:\n",
    "    # TODO: Distribution parameters may be wrong here.\n",
    "    lambda_ = pm.Normal('lambda', mu=mu_y, tau=1/var_y)\n",
    "    r_ = pm.Gamma('r', mu=1, sigma=1/var_y)\n",
    "    mu_ = pm.Normal('mu', mu=lambda_, tau=r_, shape=k)\n",
    "    beta_ = pm.InverseGamma('beta', mu=1, sigma=1)\n",
    "    omega_ = pm.Gamma('omega', alpha=1, beta=var_y)\n",
    "    s_ = pm.Gamma('s', mu=beta_, sigma=1/omega_, shape=k)\n",
    "    alpha_ = pm.Gamma('alpha', alpha=1, beta=1)\n",
    "    pi_ = pm.Dirichlet('pi', a=np.ones(k) * (alpha_ / k))\n",
    "    c_ = pm.Categorical('c', p=pi_)\n",
    "    y_ = pm.Normal('y', mu=mu_[c_], tau=s_[c_], observed=y)\n",
    "\n",
    "pm.model_to_graphviz(finiteGMMmodel)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having observed data, the parameters and the hyperparameters can be inferred by Gibbs Sampling.\n",
    "\n",
    "In the Gibbs Sampling update, the (hyper)parameters are listed in a sequence and each (hyper)parameter is updated by sampling from its distribution conditioned on all the other (hyper)parameters. Repeated application of this update ensures that in the limit, the values of the (hyper)parameters obtained arise from the joint distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\vec{\\mu}, \\vec{s}, \\vec{\\pi}, c_1, \\cdots, c_n, \\lambda, r, \\beta, \\omega, \\alpha |y_1, \\cdots, y_n)\n",
    "\\end{equation}\n",
    "\n",
    "To cover my bases, I'll assume that the $c_i$'s are known. I'll use the Gibbs Sampling updates to infer model parameters from the following distribution.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\vec{\\mu}, \\vec{s}, \\vec{\\pi}, \\lambda, r, \\beta, \\omega, \\alpha |c_1, \\cdots, c_n, y_1, \\cdots, y_n)\n",
    "\\end{equation}\n",
    "\n",
    "The conditional for the means are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\mu_j | \\vec{c}, \\vec{y}, s_j, \\lambda, r) \\sim \\mathcal{N}(\\frac{\\bar{y_j}n_js_j + \\lambda r}{n_js_j+r}, \\frac{1}{n_js_j + r})\n",
    "\\end{equation}\n",
    "\n",
    "Where, $\\bar{y_j}$ is sum of all those y's in class $j$. We compute the conditional for the means of all classes simultaneously over here. Note that only variables upon which $\\mu_j$ is conditioned on contain it's Markov Blanket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSampler (c, y, s, lambda_, r, k, **kwargs) :\n",
    "    n = np.array([np.sum(c == j) for j in range(k)])\n",
    "    y_ = np.array([np.sum(y[c == j]) / nj for j, nj in enumerate(n)])\n",
    "    mean = (y_ * n * s + lambda_ * r) / (n * s + r)\n",
    "    var = 1 / (n * s + r)\n",
    "    sample = rng.randn(k)\n",
    "    return mean + np.sqrt(var) * sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional for the precisions are: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(s_j | \\vec{c}, \\vec{y}, \\mu_j, \\beta, \\omega) \\sim \\mathcal{G}(\\beta + n_j, \\Big(\\frac{1}{\\beta + n_j}(\\omega\\beta + \\sum_{i:c_i=j}(y_i - \\mu_j)^2)\\Big)^{-1})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionSampler (c, y, mu, beta, omega, k, **kwargs) :\n",
    "    n = np.array([np.sum(c == j) for j in range(k)])\n",
    "    deltas = np.array([np.sum(np.linalg.norm(y[c == j] - mu_j)**2) for j, mu_j in enumerate(mu)])\n",
    "    shape = beta + n\n",
    "    mean = ((deltas + omega * beta) / shape) ** (-1)\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional for $\\lambda$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\lambda | \\vec{\\mu}, r) \\sim \\mathcal{N}(\\frac{\\mu_y\\sigma_y^{-2}+r\\sum_{j=1}^k\\mu_j}{\\sigma_y^{-2}+kr}, \\frac{1}{\\sigma_y^{-2}+kr})\n",
    "\\end{equation}\n",
    "\n",
    "Conditional for $r$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(r | \\vec{\\mu}, \\lambda) \\sim \\mathcal{G}(k+1, \\Big(\\frac{1}{k+1}(\\sigma_y^2 + \\sum_{j=1}^k(\\mu_j - \\lambda)^2)\\Big)^{-1})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaSampler(mu, mu_y, var_y, k, r, **kwargs) :\n",
    "    mean = (mu_y*1/var_y + r*sum(mu))/(1/var_y + k*r)\n",
    "    var = 1/(1/var_y + k*r)\n",
    "    return mean + np.sqrt(var) * rng.randn()\n",
    "\n",
    "def rSampler (mu, mu_y, var_y, k, lambda_, **kwargs) :\n",
    "    shape = k + 1\n",
    "    mean = ((1/shape)*(var_y + np.linalg.norm(mu - lambda_)**2))**-1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional for $\\beta$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\beta | \\vec{s}, \\omega) \\propto \\Gamma(\\frac{\\beta}{2})^{-k}exp(\\frac{-1}{2\\beta})(\\frac{\\beta}{2})^{(k\\beta-3)/2}\\prod_{j=1}^k(s_j\\omega)^{\\beta/2}exp(-\\frac{\\beta s_j \\omega}{2})\n",
    "\\end{equation}\n",
    "\n",
    "We'll use Adaptive Rejection Sampling to sample from $\\beta$'s. I'm confused because if we need just one sample, what is the point of using ARS. I need to check whether ARS works when given an unnormalized distribution. Maybe it is hard to get good proposals in general.\n",
    "\n",
    "Conditional for $\\omega$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\omega | \\vec{s}, \\beta) \\sim \\mathcal{G}(k\\beta+1, \\Big(\\frac{1}{k\\beta+1}(\\sigma_y^2 + \\beta\\sum_{j=1}^ks_j)\\Big)^{-1})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaSampler (s, omega, k, **kwargs) :\n",
    "    def logU (y) :\n",
    "        beta = torch.exp(y)\n",
    "        t1 = (-k * torch.lgamma(beta/2))\n",
    "        t2 = (-1/(2*beta))\n",
    "        t3 = ((k*beta-3)/2)*(y - math.log(2))\n",
    "        t4 = (beta/2)*np.sum(np.log(s*omega) - s*omega)\n",
    "        return y + t1 + t2 + t3 + t4\n",
    "    sample = adaptiveRejectionSampling(logU, 1, (-math.inf, math.inf))\n",
    "    return np.exp(sample)\n",
    "\n",
    "def omegaSampler (s, var_y, k, beta, **kwargs) :\n",
    "    shape = k*beta + 1\n",
    "    mean = ((1/shape)*(var_y + beta * sum(s)))**-1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined all the conditionals, we can do inference using Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 57.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'beta': array([0.31796712], dtype=float32),\n",
      "  'lambda_': 0.9678000046422276,\n",
      "  'mu': array([-1.99958841e+01, -1.18963417e-02,  2.00424531e+01]),\n",
      "  'omega': array([0.03029385]),\n",
      "  'r': 0.004568612495703457,\n",
      "  's': array([0.98041001, 1.0132418 , 0.92838544])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'mu': np.zeros(3),\n",
    "    's': np.ones(3),\n",
    "    'lambda_': mu_y,\n",
    "    'r': 1, \n",
    "    'beta': var_y,\n",
    "    'omega': var_y\n",
    "}\n",
    "\n",
    "samplers = {\n",
    "    'mu': meanSampler,\n",
    "    's': precisionSampler,\n",
    "    'lambda_': lambdaSampler,\n",
    "    'r': rSampler, \n",
    "    'beta': betaSampler,\n",
    "    'omega': omegaSampler\n",
    "}\n",
    "\n",
    "others = {\n",
    "    'k': 3,\n",
    "    'mu_y': mu_y,\n",
    "    'var_y': var_y,\n",
    "    'c': c,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "inferredParameters = gibbs(parameters, samplers, 10, **others)\n",
    "pprint.PrettyPrinter(indent=2).pprint(inferredParameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task was easy. Since we knew the components, the problem became analogous to fitting three independent gaussians instead of a mixture. Also, the data, once seperated on the basis of class, gave good indication of the mean and the variance for the mixture components.\n",
    "\n",
    "Now, let's suppose that the $c_i$'s are not known. Firstly, we integrate over all $\\vec{\\pi}$ so that parent of the $c_i$'s is $\\alpha$. Then, the conditional for $\\alpha$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(\\alpha | \\vec{c}) \\propto \\frac{\\alpha^{k - 3/2}exp(\\frac{-1}{2\\alpha})\\Gamma(\\alpha)}{\\Gamma(n + \\alpha)}\n",
    "\\end{equation}\n",
    "\n",
    "We'll use Adaptive Rejection Sampling to obtain samples of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaSampler (c, k, **kwargs) : \n",
    "    n = len(c)\n",
    "    def logU (y) : \n",
    "        alpha = torch.exp(y)\n",
    "        t1 = y * (k - (3/2))\n",
    "        t2 = -1/(2 * alpha)\n",
    "        t3 = torch.lgamma(alpha)\n",
    "        t4 = -torch.lgamma(n + alpha)\n",
    "        return y + t1 + t2 + t3 + t4\n",
    "    sample = adaptiveRejectionSampling(logU, 1, (-math.inf, math.inf))\n",
    "    return np.exp(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov Blanket of $c_i$ contains $y_i$, $\\vec{\\mu}$, $\\vec{s}$ and $\\alpha$. Hence, we have to find an expression for:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(c_i | y_i, \\vec{\\mu}, \\vec{s}, \\alpha) \n",
    "\\end{equation}\n",
    "\n",
    "Each $c_i$ is conditionally independent of the $\\vec{c}_{-i}$. Hence, if we condition on this variable as well, it doesn't matter.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(c_i | y_i, \\vec{\\mu}, \\vec{s}, \\alpha) = \\mathbf{Pr}(c_i | y_i, \\vec{\\mu}, \\vec{s}, \\alpha, \\vec{c}_{-i}) \n",
    "\\end{equation}\n",
    "\n",
    "Now, it is much simpler to factor the conditional probability distribution.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(c_i | y_i, \\vec{\\mu}, \\vec{s}, \\alpha, \\vec{c}_{-i}) \\propto \\mathbf{Pr}(y_i | \\vec{c}, \\vec{\\mu}, \\vec{s}, \\alpha) \\times \\mathbf{Pr}(c_i | \\vec{c}_{-i}, \\vec{\\mu}, \\vec{s}, \\alpha)\n",
    "\\end{equation}\n",
    "\n",
    "The first term is simply a Gaussian centered at $\\mu_{c_i}$ with precision $s_{c_i}$. Since, $c_i$ is conditionally independent of all non-descendants given it's parent $\\alpha$, the second term is becomes $\\mathbf{Pr}(c_i | \\vec{c}_{-i}, \\alpha)$. This has a closed form solution obtained by solving the Dirichlet Integral to integrate out $\\vec{\\pi}$. \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Pr}(c_i | \\vec{c}_{-i}, \\alpha) = \\frac{n_{-i, j} + \\frac{\\alpha}{k}}{n - 1 + \\alpha}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $n_{-i, j}$ is the number of $j$ components when data point $i$ is not included.\n",
    "\n",
    "Although according to Gibbs Sampling, we have to update the $c_i$'s sequentially, I'll update them in parallel because I don't think it makes that big a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cSampler (c, y, mu, s, alpha, k, **kwargs) :\n",
    "    n = len(c)\n",
    "    nj = np.array([np.sum(c == j) for j in range(k)])\n",
    "    nij = k\n",
    "    delta = (np.repeat(y, k, axis=1) - mu) ** 2\n",
    "    probs = ((nij + (alpha / k)) / (n - 1 + alpha)) * (s**(1/2) * np.exp(-s * delta/ 2))\n",
    "    probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "    return np.array([np.argmax(rng.multinomial(1, pvals=pvals)) for pvals in probs])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:30<00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'alpha': array([0.18565221], dtype=float32),\n",
      "  'beta': array([0.423374], dtype=float32),\n",
      "  'c': array([2, 2, 2, ..., 1, 1, 1]),\n",
      "  'lambda_': -0.06477585202582059,\n",
      "  'mu': array([  0.03802339,  20.02062615, -14.51761252]),\n",
      "  'omega': array([0.01215156]),\n",
      "  'r': 0.0008575192960488669,\n",
      "  's': array([1.21340523, 0.94654014, 0.01166924])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'alpha': 1,\n",
    "    'mu': np.zeros(3),\n",
    "    's': np.ones(3),\n",
    "    'lambda_': mu_y,\n",
    "    'r': 1, \n",
    "    'beta': var_y,\n",
    "    'omega': var_y,\n",
    "    'c': np.array([np.random.randint(0, k) for _ in range(n)])\n",
    "}\n",
    "\n",
    "samplers = {\n",
    "    'alpha': alphaSampler,\n",
    "    'c': cSampler,\n",
    "    'mu': meanSampler,\n",
    "    's': precisionSampler,\n",
    "    'lambda_': lambdaSampler,\n",
    "    'r': rSampler, \n",
    "    'beta': betaSampler,\n",
    "    'omega': omegaSampler\n",
    "}\n",
    "\n",
    "others = {\n",
    "    'k': 3,\n",
    "    'mu_y': mu_y,\n",
    "    'var_y': var_y,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "inferredParameters = gibbs(parameters, samplers, 200, **others)\n",
    "pprint.PrettyPrinter(indent=2).pprint(inferredParameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the means and the precisions right! Now, we need to take it to the infinite limit. The extension is quite straightforward to implement but a bit difficult to derive. \n",
    "\n",
    "In the beginning, none of the data points $y_i$ will have a component associated with them. They will be associated with a pseudo-component corresponding to all the unrepresented components. \n",
    "\n",
    "During the sampling of the indicator variables, for each $y_i$, it'll be decided whether the corresponding component is one of the finite components made so far or whether the component is one of the infinite unrepresented components. If an unrepresented component is chosen for the $y_i$, then a new class label is added and the mean and precision for this new class label is added. \n",
    "\n",
    "At any point, if there is a component which has no $y_i$ associated with it, then that component, along with it's parameters is discarded.\n",
    "\n",
    "In this manner, the algorithm adaptively chooses $k$, the correct number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "def unrepresentedMeanSampler (lambda_, r, **kwargs) : \n",
    "    var = 1 / r\n",
    "    return lambda_ + np.sqrt(var) * rng.randn()\n",
    "\n",
    "def unrepresentedPrecisionSampler (beta, omega, **kwargs) :\n",
    "    shape = beta\n",
    "    mean = 1 / omega\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n",
    "def cSampler (c, y, mu, s, unrepresentedMu, unrepresentedPrecision, alpha, lambda_, r, beta, omega, **kwargs):\n",
    "    global k\n",
    "    s_, mu_ = np.array(s), np.array(mu)\n",
    "    n = len(c)\n",
    "    nij = (n - np.array([c == j for j in range(k)])).T\n",
    "    delta1 = (np.repeat(y, k, axis=1) - mu) ** 2\n",
    "    delta2 = (y - unrepresentedMu) ** 2  \n",
    "    #print(delta1, delta2)\n",
    "    probs1 = (nij / (n - 1 + alpha)) * (np.sqrt(s_/(2*np.pi)) * np.exp(-s_ * delta1 / 2))\n",
    "    likelihood = np.sqrt(unrepresentedPrecision) * np.exp(-unrepresentedPrecision * delta2 / 2) \n",
    "    probs2 = (alpha / (n - 1 + alpha)) * likelihood\n",
    "    # Probabilities of the existing components concatenated with probability\n",
    "    # for all the other components put together.\n",
    "    probs = np.concatenate((probs1, probs2), axis=1)\n",
    "    probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "    # New Components sampled using those probabilities\n",
    "    newC = np.array([np.argmax(rng.multinomial(1, pvals=pvals)) for pvals in probs])\n",
    "    new_nj = np.array([np.sum(c == j) for j in range(k)])\n",
    "    # Time to remove all those components who have no \n",
    "    # data point associated with them anymore!\n",
    "    notClassLabels = np.array(range(k))[new_nj == 0]\n",
    "    for label in notClassLabels : \n",
    "        newC[newC == label] = -1\n",
    "    # Re-index the remaining component labels.\n",
    "    classLabels = np.array(range(k))[new_nj > 0]\n",
    "    mu.clear()\n",
    "    mu.extend(mu_[new_nj > 0].tolist())\n",
    "    s.clear()\n",
    "    s.extend(s_[new_nj > 0].tolist())\n",
    "    newK = sum(new_nj > 0)\n",
    "    for i, label in enumerate(classLabels) :\n",
    "        newC[newC == label] = i\n",
    "    # If any data point is assigned one of the \n",
    "    # unrepresented classes, initialize a new class\n",
    "    # with parameters mean and precision. \n",
    "    if sum(newC == k) > 0 : \n",
    "        newC[newC == k] = newK\n",
    "        mu.append(unrepresentedMu)\n",
    "        s.append(unrepresentedPrecision)\n",
    "        k = newK + 1\n",
    "    return newC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## Redefining the previous samplers so that they can\n",
    "## work with varying number of components.\n",
    "######################################################\n",
    "\n",
    "def meanSampler (c, y, s, lambda_, r, **kwargs) :\n",
    "    n = np.array([np.sum(c == j) for j in range(k)])\n",
    "    y_ = np.array([np.sum(y[c == j]) / nj for j, nj in enumerate(n)])\n",
    "    mean = (y_ * n * s + lambda_ * r) / (n * s + r)\n",
    "    var = 1 / (n * s + r)\n",
    "    sample = rng.randn(k)\n",
    "    return (mean + np.sqrt(var) * sample).tolist()\n",
    "\n",
    "def precisionSampler (c, y, mu, beta, omega, **kwargs) :\n",
    "    n = np.array([np.sum(c == j) for j in range(k)])\n",
    "    deltas = np.array([np.sum(np.linalg.norm(y[c == j] - mu_j)**2) for j, mu_j in enumerate(mu)])\n",
    "    shape = beta + n\n",
    "    mean = ((deltas + omega * beta) / shape) ** (-1)\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale).tolist()\n",
    "\n",
    "def lambdaSampler(mu, mu_y, var_y, r, **kwargs) :\n",
    "    mean = (mu_y*1/var_y + r*sum(mu))/(1/var_y + k*r)\n",
    "    var = 1/(1/var_y + k*r)\n",
    "    return mean + np.sqrt(var) * rng.randn()\n",
    "\n",
    "def rSampler (mu, mu_y, var_y, lambda_, **kwargs) :\n",
    "    shape = k + 1\n",
    "    mean = ((1/shape)*(var_y + np.linalg.norm(np.array(mu) - lambda_)**2))**-1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n",
    "def betaSampler (s, omega, **kwargs) :\n",
    "    s_ = np.array(s)\n",
    "    def logU (y) :\n",
    "        if k == 0 : \n",
    "            return y + (-3/2) * (y - math.log(2)) - (1/2) * torch.exp(-y)\n",
    "        beta = torch.exp(y)\n",
    "        t1 = (-k * torch.lgamma(beta/2))\n",
    "        t2 = (-1/(2*beta))\n",
    "        t3 = ((k*beta-3)/2)*(y - math.log(2))\n",
    "        t4 = (beta/2)*np.sum(np.log(s_*omega) - s_*omega)\n",
    "        return y + t1 + t2 + t3 + t4\n",
    "    sample = adaptiveRejectionSampling(logU, 1, (-math.inf, math.inf)).pop()\n",
    "    return np.exp(float(sample))\n",
    "\n",
    "def omegaSampler (s, var_y, beta, **kwargs) :\n",
    "    shape = k*beta + 1\n",
    "    mean = ((1/shape)*(var_y + beta * sum(s)))**-1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n",
    "def alphaSampler (c, **kwargs) :\n",
    "    n = len(c)\n",
    "    def logU (y) :\n",
    "        alpha = torch.exp(y)\n",
    "        t1 = y * (k - (3/2))\n",
    "        t2 = -1/(2 * alpha)\n",
    "        t3 = torch.lgamma(alpha)\n",
    "        t4 = -torch.lgamma(n + alpha)\n",
    "        return y + t1 + t2 + t3 + t4\n",
    "    sample = adaptiveRejectionSampling(logU, 1, (-math.inf, math.inf)).pop()\n",
    "    return np.exp(float(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'alpha': 0.2026171387747238,\n",
      "  'beta': 0.2203269933359313,\n",
      "  'c': array([0, 0, 0, ..., 0, 0, 0]),\n",
      "  'lambda_': -6.9706923880237515,\n",
      "  'mu': [0],\n",
      "  'omega': 0.002998988136705314,\n",
      "  'r': 0.018696438924689536,\n",
      "  's': [1],\n",
      "  'unrepresentedMu': -14.21119506636325,\n",
      "  'unrepresentedPrecision': 868.9177683900873}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'alpha': 1,\n",
    "    'mu': [],\n",
    "    's': [],\n",
    "    'lambda_': mu_y,\n",
    "    'r': 1, \n",
    "    'beta': var_y,\n",
    "    'omega': var_y,\n",
    "    'c': np.ones(n) * -1,\n",
    "    'unrepresentedMu': 0,\n",
    "    'unrepresentedPrecision': 1\n",
    "}\n",
    "\n",
    "samplers = {\n",
    "    'alpha': alphaSampler,\n",
    "    'c': cSampler,\n",
    "    'mu': meanSampler,\n",
    "    's': precisionSampler,\n",
    "    'lambda_': lambdaSampler,\n",
    "    'r': rSampler, \n",
    "    'beta': betaSampler,\n",
    "    'omega': omegaSampler,\n",
    "    'unrepresentedMu': unrepresentedMeanSampler,\n",
    "    'unrepresentedPrecision': unrepresentedPrecisionSampler\n",
    "}\n",
    "\n",
    "others = {\n",
    "    'mu_y': mu_y,\n",
    "    'var_y': var_y,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "inferredParameters = gibbs(parameters, samplers, 1, **others)\n",
    "pprint.PrettyPrinter(indent=2).pprint(inferredParameters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of classes are too many. But the mean and precisions are still accurate. Hence, if we were to sample from the model given by the inferred parameters, the samples will resemble those from the actual distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "\n",
    "def nCRP(alpha, nCustomers):\n",
    "    # Initialize top-level restaurant\n",
    "    top_restaurant = [0]\n",
    "    top_restaurant_tables = [[0]]\n",
    "\n",
    "    # Initialize bottom-level restaurants\n",
    "    bottom_restaurants = [[0]]\n",
    "\n",
    "    # Assign customers to top-level restaurant\n",
    "    top_assignments = [0]\n",
    "\n",
    "    for i in range(1, nCustomers):\n",
    "        # Probability of joining existing tables in the top-level restaurant\n",
    "        top_probs = [len(table) / (i + alpha) for table in top_restaurant_tables]\n",
    "\n",
    "        # Probability of starting a new table in the top-level restaurant\n",
    "        top_probs.append(alpha / (i + alpha))\n",
    "\n",
    "        # Sample table assignment in the top-level restaurant\n",
    "        top_table = np.random.choice(len(top_restaurant_tables) + 1, p=top_probs)\n",
    "\n",
    "        if top_table == len(top_restaurant_tables):\n",
    "            # Start a new table in the top-level restaurant\n",
    "            top_restaurant_tables.append([i])\n",
    "        else:\n",
    "            # Assign to an existing table in the top-level restaurant\n",
    "            top_restaurant_tables[top_table].append(i)\n",
    "\n",
    "        # Assign the customer to the chosen top-level restaurant table\n",
    "        top_assignments.append(top_table)\n",
    "\n",
    "        # Sample table assignment in the bottom-level restaurant\n",
    "        bottom_restaurant = top_assignments[i]\n",
    "\n",
    "        if bottom_restaurant == len(bottom_restaurants):\n",
    "            # Start a new bottom-level restaurant\n",
    "            bottom_restaurants.append([i])\n",
    "        else:\n",
    "            # Assign to an existing bottom-level restaurant\n",
    "            bottom_restaurants[bottom_restaurant].append(i)\n",
    "\n",
    "    return top_assignments, top_restaurant_tables, bottom_restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSampler(c, y, s, bottom_restaurants, **kwargs):\n",
    "    k = len(s)\n",
    "    mu = []\n",
    "    for j in range(k):\n",
    "        mu_j = np.mean(y[c == j]) if np.sum(c == j) > 0 else 0\n",
    "        bottom_mu_j = [mu_i for mu_i, r_i in bottom_restaurants if r_i == j]\n",
    "        if len(bottom_mu_j) > 0:\n",
    "            mu_j += np.mean(bottom_mu_j)\n",
    "        mu.append(mu_j)\n",
    "    return mu\n",
    "\n",
    "def precisionSampler(c, y, mu, bottom_restaurants, **kwargs):\n",
    "    k = len(mu)\n",
    "    s = []\n",
    "    for j in range(k):\n",
    "        s_j = np.var(y[c == j]) if np.sum(c == j) > 0 else 1\n",
    "        bottom_s_j = [s_i for s_i, r_i in bottom_restaurants if r_i == j]\n",
    "        if len(bottom_s_j) > 0:\n",
    "            s_j += np.var(bottom_s_j)\n",
    "        s.append(s_j)\n",
    "    return s\n",
    "\n",
    "def unrepresentedMeanSampler (lambda_, r, **kwargs) : \n",
    "    var = 1 / r\n",
    "    return lambda_ + np.sqrt(var) * rng.randn()\n",
    "\n",
    "def unrepresentedPrecisionSampler(beta, omega, **kwargs):\n",
    "    shape = beta\n",
    "    mean = 1 / omega\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaSampler(mu, mu_y, var_y, r, **kwargs):\n",
    "    k = len(mu)\n",
    "    #nj = np.array([np.sum(c == j) for j in range(k)])  # Assuming 'c' is the cluster assignment variable\n",
    "    mean = (mu_y * (1/var_y) + r * np.sum(mu)) / (var_y + k * r)\n",
    "    var = 1 / (1/var_y + k * r)\n",
    "    return mean + np.sqrt(var) * rng.randn()\n",
    "\n",
    "def rSampler(mu, mu_y, var_y, lambda_, **kwargs):\n",
    "    k = len(mu)\n",
    "    shape = k + 1\n",
    "    mean = ((1 / shape) * (var_y + np.sum((mu - lambda_) ** 2))) ** -1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n",
    "def betaSampler(s, omega, bottom_restaurants, **kwargs):\n",
    "    k = len(s)\n",
    "    beta = 1\n",
    "    for j in range(k):\n",
    "        s_j = s[j]\n",
    "        bottom_s_j = [s_i for s_i, r_i in bottom_restaurants if r_i == j]\n",
    "        if len(bottom_s_j) > 0:\n",
    "            s_j += np.sum(bottom_s_j)\n",
    "        beta += np.sum(s_j)\n",
    "    return beta\n",
    "\n",
    "def omegaSampler(s, var_y, beta, bottom_restaurants, **kwargs):\n",
    "    k = len(s)\n",
    "    shape = k * beta + 1\n",
    "    # mean = ((1 / shape) * (var_y + beta * np.sum(s))) ** -1\n",
    "    mean = ((1 / shape) * (var_y + beta * np.sum(s) + np.sum([np.sum(np.array(bottom_restaurants)[:, 0][c == j]) for j in range(k)]))) ** -1\n",
    "    scale = mean / shape\n",
    "    return rng.gamma(shape, scale)\n",
    "\n",
    "def alphaSampler(c, alpha, top_restaurant_tables, **kwargs):\n",
    "    n = len(c)\n",
    "    alpha = alpha + len(top_restaurant_tables)\n",
    "    return rng.gamma(alpha, 1.0 / (n + alpha))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cSampler(c, y, mu, s, alpha, top_assignments, top_restaurant_tables, bottom_restaurants, unrepresentedMu, unrepresentedPrecision, **kwargs):\n",
    "    n = len(c)\n",
    "    #nj = np.array([np.sum(c == j) for j in range(k)])\n",
    "    k = len(mu)\n",
    "    nij = np.array([np.sum(c[bottom_restaurants[i]] == j) for i in range(len(bottom_restaurants)) for j in range(k)])\n",
    "    delta1 = (np.repeat(y, k, axis=1) - np.array(mu)) ** 2\n",
    "    delta2 = (y - unrepresentedMu) ** 2\n",
    "    print(delta1, delta2)\n",
    "\n",
    "    # Calculate the probabilities of cluster assignments based on nCRP\n",
    "    probs = []\n",
    "    for i in range(n):\n",
    "        p = []\n",
    "        p_new = alpha / (n - 1 + alpha)\n",
    "\n",
    "        for j in range(k):\n",
    "            # Probability of assigning to an existing bottom-level restaurant\n",
    "            p_existing = (nij[bottom_restaurants.index(top_assignments[i])][j] + (alpha / k)) / (n - 1 + alpha)\n",
    "            p_val = p_existing * np.sqrt(s[j]) * np.exp(-s[j] * delta1[i, j] / 2)\n",
    "            p.append(p_val if p_val >= 0 else 0)\n",
    "            print('k=====', k, '    pval', p_val)\n",
    "        \n",
    "        p_val = p_new * np.sqrt(unrepresentedPrecision) * np.exp(-unrepresentedPrecision * delta2[i] / 2)\n",
    "        #print('pval  ', p_val,' 222 ', np.sqrt(unrepresentedPrecision), ' 333 ', delta2[i], ' 333 ', np.exp(-unrepresentedPrecision * delta2[i] / 2))\n",
    "        p.append(p_val if p_val >= 0 else 0)\n",
    "\n",
    "        p = [val / sum(p) for val in p]\n",
    "\n",
    "        probs.append(p)\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    new_c = np.array([rng.choice(np.flatnonzero(p == p.max())) for p in probs])\n",
    "    '''probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "\n",
    "    # Sample new cluster assignments\n",
    "    #print(probs)\n",
    "    new_c = np.array([np.argmax(rng.multinomial(1, pvals=pvals.flatten())) for pvals in probs])'''\n",
    "\n",
    "    return new_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1  # Concentration parameter\n",
    "nCustomers = len(y)  # Number of customers/data points\n",
    "top_assignments, top_restaurant_tables, bottom_restaurants = nCRP(alpha, nCustomers)\n",
    "\n",
    "parameters = {\n",
    "    'alpha': alpha,\n",
    "    'mu': [],\n",
    "    's': [],\n",
    "    'lambda_': mu_y,\n",
    "    'r': 1, \n",
    "    'beta': var_y,\n",
    "    'omega': var_y,\n",
    "    'c': np.ones(n) * -1,\n",
    "    'unrepresentedMu': 0,\n",
    "    'unrepresentedPrecision': 1\n",
    "}\n",
    "\n",
    "samplers = {\n",
    "    'alpha': alphaSampler,\n",
    "    'c': cSampler,\n",
    "    'mu': meanSampler,\n",
    "    's': precisionSampler,\n",
    "    'lambda_': lambdaSampler,\n",
    "    'r': rSampler, \n",
    "    'beta': betaSampler,\n",
    "    'omega': omegaSampler,\n",
    "    'unrepresentedMu': unrepresentedMeanSampler,\n",
    "    'unrepresentedPrecision': unrepresentedPrecisionSampler,\n",
    "}\n",
    "\n",
    "others = {\n",
    "    'mu_y': mu_y,\n",
    "    'var_y': var_y,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "def gibbs(x, samplers, nSteps, **kwargs):\n",
    "    for i in tqdm(range(nSteps)):\n",
    "        # Update nCRP at each iteration\n",
    "        top_assignments, top_restaurant_tables, bottom_restaurants = nCRP(x['alpha'], len(y))\n",
    "\n",
    "        for k, v in x.items():\n",
    "            if k in samplers:\n",
    "                x[k] = samplers[k](**x, top_assignments=top_assignments, top_restaurant_tables=top_restaurant_tables, bottom_restaurants=bottom_restaurants, **kwargs)\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:00<01:03,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [[386.11891994]\n",
      " [366.39848225]\n",
      " [458.80458672]\n",
      " ...\n",
      " [363.86889323]\n",
      " [467.52422543]\n",
      " [351.90971839]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_2/d0xmq0nn39z8s3ccvbbr049h0000gn/T/ipykernel_34361/3209585779.py:27: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = [val / sum(p) for val in p]\n",
      "  0%|          | 1/200 [00:00<01:48,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [[2359.18678067]\n",
      " [2310.06060018]\n",
      " [2534.24405902]\n",
      " ...\n",
      " [  96.94773056]\n",
      " [  53.27895042]\n",
      " [ 103.27223522]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_2/d0xmq0nn39z8s3ccvbbr049h0000gn/T/ipykernel_34361/4110702548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call the modified gibbs function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minferredParameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgibbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrettyPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minferredParameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_2/d0xmq0nn39z8s3ccvbbr049h0000gn/T/ipykernel_34361/370819084.py\u001b[0m in \u001b[0;36mgibbs\u001b[0;34m(x, samplers, nSteps, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamplers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamplers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_assignments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_assignments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_restaurant_tables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_restaurant_tables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom_restaurants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom_restaurants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_2/d0xmq0nn39z8s3ccvbbr049h0000gn/T/ipykernel_34361/3209585779.py\u001b[0m in \u001b[0;36mcSampler\u001b[0;34m(c, y, mu, s, alpha, top_assignments, top_restaurant_tables, bottom_restaurants, unrepresentedMu, unrepresentedPrecision, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mnew_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     '''probs = probs / np.sum(probs, axis=1, keepdims=True)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_2/d0xmq0nn39z8s3ccvbbr049h0000gn/T/ipykernel_34361/3209585779.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mnew_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     '''probs = probs / np.sum(probs, axis=1, keepdims=True)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the modified gibbs function\n",
    "inferredParameters = gibbs(parameters, samplers, 200, **others)\n",
    "pprint.PrettyPrinter(indent=2).pprint(inferredParameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
